{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdaea8b-87cd-48e8-9617-59cc34dac6c2",
   "metadata": {
    "id": "2fdaea8b-87cd-48e8-9617-59cc34dac6c2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e47445d-9ffd-4299-8b01-24e0ad91a8b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7e47445d-9ffd-4299-8b01-24e0ad91a8b9",
    "outputId": "bc0e6f1a-8612-4ec2-e16a-5669495a15db",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a78666-5a35-4482-a63d-955b28bcbe1d",
   "metadata": {
    "id": "16a78666-5a35-4482-a63d-955b28bcbe1d",
    "tags": []
   },
   "source": [
    "# Linear Layer ([Documentation](http://pytorch.org/docs/stable/generated/torch.nn.Linear.html))\n",
    "``torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)``\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dde346-98c7-4e75-9983-bfac4f9e9f64",
   "metadata": {
    "id": "15dde346-98c7-4e75-9983-bfac4f9e9f64"
   },
   "source": [
    "Let us start with a single neuron having 10 inputs and just 1 output, but no bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cbb283-dec1-4100-904b-ec08e83c3a5a",
   "metadata": {
    "id": "37cbb283-dec1-4100-904b-ec08e83c3a5a"
   },
   "source": [
    "## Single Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e17a41-7224-4517-b5da-872702319b4e",
   "metadata": {
    "id": "38e17a41-7224-4517-b5da-872702319b4e"
   },
   "source": [
    "Let us start with a single neuron having 10 input features and just 1 output feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87113a2e-a4bc-4b73-8dd7-e82c0047fac0",
   "metadata": {
    "id": "87113a2e-a4bc-4b73-8dd7-e82c0047fac0"
   },
   "outputs": [],
   "source": [
    "single_neuron = nn.Linear()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62db7b4f-9d18-402d-9137-bbe2a31e1f3f",
   "metadata": {
    "id": "62db7b4f-9d18-402d-9137-bbe2a31e1f3f"
   },
   "source": [
    "PyTorch automatically ininitializes the weights of all layers, we can look at those weights by calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb93730-ae0f-4f50-8a68-7d5acf750337",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "abb93730-ae0f-4f50-8a68-7d5acf750337",
    "outputId": "1e1ce798-f910-4dca-a092-05decf79daee"
   },
   "outputs": [],
   "source": [
    "print(single_neuron.weight.shape)\n",
    "print(single_neuron.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf46c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_neuron.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c4080d-d044-4dd0-b061-2ad0c84b9c1e",
   "metadata": {
    "id": "81c4080d-d044-4dd0-b061-2ad0c84b9c1e"
   },
   "source": [
    "An All-Zero Output always evaluates to 0 when there is no bias. Let us verify this by creating a suitable input tensor of ``(b, input_features) == (1, 10)`` with ``b`` being the batch size, ``input_features`` being the number of input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba59bfb-4fdd-4455-834f-b7014f3d3949",
   "metadata": {
    "id": "eba59bfb-4fdd-4455-834f-b7014f3d3949"
   },
   "outputs": [],
   "source": [
    "input_0 = torch.zeros((1, 10))\n",
    "print(input_0.shape)\n",
    "input_1 = torch.ones((1, 10))\n",
    "print(input_1)\n",
    "input_1000 = 1000 * torch.ones((1, 10))\n",
    "print(input_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750694c7-4a04-4329-9e23-9c05799690e5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "750694c7-4a04-4329-9e23-9c05799690e5",
    "outputId": "0ac03e5b-009d-4734-9374-e75c4fb58fdc"
   },
   "outputs": [],
   "source": [
    "print(single_neuron(input_0))\n",
    "print(single_neuron(input_1))\n",
    "print(single_neuron(input_1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mHfA2p6e0_ox",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mHfA2p6e0_ox",
    "outputId": "294a61e1-bd18-45c2-a4f5-4a3eff6eb3fb"
   },
   "outputs": [],
   "source": [
    "# check the sum of weights\n",
    "print(torch.sum(single_neuron.weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42232bd0-791b-46d0-9cd7-fc9236b2ab05",
   "metadata": {
    "id": "42232bd0-791b-46d0-9cd7-fc9236b2ab05"
   },
   "source": [
    "## Linear Layer with 4 input features and 3 output features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9114dd0b-8ab8-4a5c-ac44-6eeb42443b52",
   "metadata": {
    "id": "9114dd0b-8ab8-4a5c-ac44-6eeb42443b52"
   },
   "outputs": [],
   "source": [
    "linear_layer = nn.Linear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff14e48-9ac3-43b8-8327-41991bf6d4c9",
   "metadata": {
    "id": "5ff14e48-9ac3-43b8-8327-41991bf6d4c9"
   },
   "outputs": [],
   "source": [
    "linear_input = torch.rand((1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46fcc0e-21e9-4a58-ba90-e23692f572bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b46fcc0e-21e9-4a58-ba90-e23692f572bd",
    "outputId": "15eb3f03-4af6-4289-89c1-4cdaae44f6a8"
   },
   "outputs": [],
   "source": [
    "out_feats = linear_layer(linear_input)\n",
    "print(out_feats.shape)\n",
    "linear_layer.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54e21ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer.bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662e52f0-7ea1-44c9-8b59-c9cec3f83e74",
   "metadata": {
    "id": "662e52f0-7ea1-44c9-8b59-c9cec3f83e74"
   },
   "source": [
    "A linear layer is applied to an input tensor by applying a linear transformation of the form $y = xA^T + b$. This can be easily varified by evaluating the equation manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e9e5dc-cae6-49c2-8756-da1a54c0f2a7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7e9e5dc-cae6-49c2-8756-da1a54c0f2a7",
    "outputId": "677e22e7-d52e-4065-f6a3-8549b59dbde0"
   },
   "outputs": [],
   "source": [
    "torch.mm(linear_input, linear_layer.weight.T) +linear_layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7QLU9bnn1odT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 549
    },
    "id": "7QLU9bnn1odT",
    "outputId": "e5db55d6-5a5a-4bdf-ba9c-9bcdd41a628d"
   },
   "outputs": [],
   "source": [
    "# easy to produce errors with wrong layout\n",
    "print( linear_layer.bias.shape)\n",
    "print( torch.mm(linear_layer.weight, linear_input.T) + linear_layer.bias.T )\n",
    "print( torch.mm(linear_layer.weight, linear_input.T) + linear_layer.bias)\n",
    "print( torch.mm(linear_layer.weight, linear_input.T) + linear_layer.bias.reshape(5,1) ) #only working line of code\n",
    "\n",
    "print( torch.mm(linear_layer.weight, linear_input) + linear_layer.bias)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a_3fkgG82Qfa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a_3fkgG82Qfa",
    "outputId": "94032a6c-b3c1-4ba5-d342-79dba50e075a"
   },
   "outputs": [],
   "source": [
    "# numpy verification\n",
    "result = np.dot(linear_input.detach().numpy(), linear_layer.weight.T.detach().numpy()) +linear_layer.bias.reshape(3).detach().numpy()\n",
    "print('Result: \\n', result.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaaa2c5-be60-4bd1-8fb2-2db4b3078933",
   "metadata": {
    "id": "cbaaa2c5-be60-4bd1-8fb2-2db4b3078933"
   },
   "source": [
    "## Multi Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29b961f-e8e3-460b-951b-cda531f790e3",
   "metadata": {
    "id": "b29b961f-e8e3-460b-951b-cda531f790e3"
   },
   "source": [
    "A Neural Network consisting of more than one linear layer (also referred to as Fully Connected Layer) is also called a Multi Layer Perceptron (MLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f8a763-a458-4a1e-b4e0-d55615cd0493",
   "metadata": {
    "id": "a6f8a763-a458-4a1e-b4e0-d55615cd0493"
   },
   "outputs": [],
   "source": [
    "# often a neural network is defined as object derived from \"nn.Module\"\n",
    "# we would like to define an \"__init__\" and \"forward\" function\n",
    "\n",
    "class ThreeLayerMLP(nn.Module):\n",
    "    def __init__(self, input_features, hidden_features, output_features):\n",
    "        super(ThreeLayerMLP, self).__init__()\n",
    "        self.layer_0 = nn.Linear(in_features = input_features, out_features = hidden_features)\n",
    "        self.layer_1 = nn.Linear(in_features = hidden_features, out_features = hidden_features)\n",
    "        self.layer_2 = nn.Linear(in_features = hidden_features, out_features = output_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer_0(x)\n",
    "        x = self.layer_1(x)\n",
    "        x = self.layer_2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb236539-8c01-4b8b-aa8f-70dbc69c223a",
   "metadata": {
    "id": "cb236539-8c01-4b8b-aa8f-70dbc69c223a"
   },
   "outputs": [],
   "source": [
    "# initialization\n",
    "my_mlp = ThreeLayerMLP(10, 5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a59058f-a8ce-46e9-804c-fc45cb49377c",
   "metadata": {
    "id": "5a59058f-a8ce-46e9-804c-fc45cb49377c"
   },
   "outputs": [],
   "source": [
    "mlp_input = torch.rand((1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2837fe-98c3-4b29-93bc-ff51781b68f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "df2837fe-98c3-4b29-93bc-ff51781b68f6",
    "outputId": "17593bf7-79b5-4d1d-9783-e4db05209d3f"
   },
   "outputs": [],
   "source": [
    "my_mlp(mlp_input) # this calls the forward function; handles distribution on multiple GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pCVvaf635ryz",
   "metadata": {
    "id": "pCVvaf635ryz"
   },
   "outputs": [],
   "source": [
    "# add activation functions\n",
    "# \n",
    "\n",
    "class ThreeLayerMLPNew(nn.Module):\n",
    "    def __init__(self, input_features, hidden_features, output_features):\n",
    "        super(ThreeLayerMLPNew, self).__init__()\n",
    "        self.layer_0 = nn.Linear(in_features = input_features, out_features = hidden_features)\n",
    "        self.layer_1 = nn.Linear(in_features = hidden_features, out_features = hidden_features)\n",
    "        self.layer_2 = nn.Linear(in_features = hidden_features, out_features = output_features)\n",
    "        self.relu = nn.ReLU() # only defined once not three times / depends on coding style / better to define layer for each use\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_0(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4KwNxZ-6icE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4KwNxZ-6icE",
    "outputId": "1b497117-dd2b-43ca-b859-4dbe0c5acefd"
   },
   "outputs": [],
   "source": [
    "# execute multiple times to see that output never gets < 0\n",
    "\n",
    "my_mlpnew = ThreeLayerMLPNew(3, 3, 1)\n",
    "mlp_input = torch.rand((1, 3))\n",
    "my_mlpnew(mlp_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae360e61-058f-4204-ae1b-5b67083e5005",
   "metadata": {
    "id": "ae360e61-058f-4204-ae1b-5b67083e5005"
   },
   "source": [
    "# Conv2d Layer ([Documentation](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html))\n",
    "``torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c074bcf-e7fe-412c-9ab8-bd25d1b6eac3",
   "metadata": {
    "id": "7c074bcf-e7fe-412c-9ab8-bd25d1b6eac3"
   },
   "outputs": [],
   "source": [
    "conv_input = torch.randn(1, 3, 9, 9) # batch_size == 1, num_channels == 3, height == 9, width == 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07f7615-3902-47bf-906b-06c128bb47b7",
   "metadata": {
    "id": "c07f7615-3902-47bf-906b-06c128bb47b7"
   },
   "outputs": [],
   "source": [
    "# With square 3x3 kernels\n",
    "m = nn.Conv2d(3, 1, 3, stride = 1, padding = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ad7a97-ddd9-4bd5-ad28-16ac9c6e34e6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "14ad7a97-ddd9-4bd5-ad28-16ac9c6e34e6",
    "outputId": "7a884980-39fe-4bf4-9c79-c234b14b0200"
   },
   "outputs": [],
   "source": [
    "output = m(conv_input)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ce155a-c89b-471d-9d1e-166491b3c73e",
   "metadata": {
    "id": "44ce155a-c89b-471d-9d1e-166491b3c73e"
   },
   "outputs": [],
   "source": [
    "# With square 3x3 kernels and stride 2\n",
    "m = nn.Conv2d(3, 1, 3, stride =  2, padding = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0df43ac-4a5d-4632-a388-94e7a477b3cd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0df43ac-4a5d-4632-a388-94e7a477b3cd",
    "outputId": "3faab113-2264-4d7a-f89a-a48434d5f034"
   },
   "outputs": [],
   "source": [
    "output = m(conv_input)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98b0184-d73f-404d-afef-0df4b9ba936b",
   "metadata": {
    "id": "d98b0184-d73f-404d-afef-0df4b9ba936b"
   },
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df9b2a6-fcc4-415f-a3ec-990c033354cd",
   "metadata": {
    "id": "3df9b2a6-fcc4-415f-a3ec-990c033354cd"
   },
   "source": [
    "A Neural Network that only consists of convolutional layers is also referred to as a convolutional neural network. The big advantage is that the input size can vary as it only has to have the right number of input channels but the spatial height and width can vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b211cf96-c24a-46c5-be48-093641fcd09c",
   "metadata": {
    "id": "b211cf96-c24a-46c5-be48-093641fcd09c"
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, output_channels):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer_0 = nn.Conv2d(in_channels = input_channels, out_channels = hidden_channels, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.relu0 = nn.ReLU()\n",
    "        self.layer_1 = nn.Conv2d(in_channels = hidden_channels, out_channels = hidden_channels, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.layer_2 = nn.Conv2d(in_channels = hidden_channels, out_channels = output_channels, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_0(x)\n",
    "        x = self.relu0( x )\n",
    "        x = self.layer_1(x)\n",
    "        x = self.relu1( x )\n",
    "        x = self.layer_2(x)\n",
    "        x = self.relu2( x )\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0261ce31-a619-4a5e-8a62-c7ada72db655",
   "metadata": {},
   "source": [
    "# TODO\n",
    "* Randomly initialise weight\n",
    "* Implement forward propagation to get ) for any\n",
    "* Implement backprop to compute partial derivatives\n",
    "* For all the samples, perform forward propagation and\n",
    "backpropagation\n",
    "* Using numerical estimation of gradient to check the gradient\n",
    "calculation, disable after checking\n",
    "* Use gradient descent or advanced optimization method with\n",
    "backpropagation to try to minimize cost function"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
